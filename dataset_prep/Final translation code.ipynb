{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U transformers","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/facebook/nllb-200-distilled-600M\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/facebook/nllb-200-distilled-600M)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{}},{"cell_type":"code","source":"print(\"kale\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T17:13:38.188460Z","iopub.execute_input":"2025-10-08T17:13:38.188750Z","iopub.status.idle":"2025-10-08T17:13:38.192987Z","shell.execute_reply.started":"2025-10-08T17:13:38.188729Z","shell.execute_reply":"2025-10-08T17:13:38.192352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NLLB Nepali Translation - Pipeline Method (Simple & Easy)\n\n# import torch\n# from transformers import pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T16:55:42.715628Z","iopub.execute_input":"2025-09-28T16:55:42.715986Z","iopub.status.idle":"2025-09-28T16:56:05.988943Z","shell.execute_reply.started":"2025-09-28T16:55:42.715964Z","shell.execute_reply":"2025-09-28T16:56:05.988386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# # Check if CUDA (GPU) is available\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# if torch.cuda.is_available():\n#     print(f\"GPU device count: {torch.cuda.device_count()}\")\n#     print(f\"Current GPU: {torch.cuda.get_device_name()}\")\n#     print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n# else:\n#     print(\"No GPU available, using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T17:13:41.762683Z","iopub.execute_input":"2025-10-08T17:13:41.763380Z","iopub.status.idle":"2025-10-08T17:13:45.551621Z","shell.execute_reply.started":"2025-10-08T17:13:41.763342Z","shell.execute_reply":"2025-10-08T17:13:45.550820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install IndicTransToolkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T17:13:47.645285Z","iopub.execute_input":"2025-10-08T17:13:47.646179Z","iopub.status.idle":"2025-10-08T17:13:53.213371Z","shell.execute_reply.started":"2025-10-08T17:13:47.646151Z","shell.execute_reply":"2025-10-08T17:13:53.212641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n# import time\n\n# start = time.time()\n# # recommended to run this on a gpu with flash_attn installed\n# # don't set attn_implemetation if you don't have flash_attn\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# # model_name = \"ai4bharat/indictrans2-en-indic-1B\"\n# # model_name = \"ai4bharat/IndicTrans3-beta\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name, \n#     trust_remote_code=True, \n#     torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n#     attn_implementation=\"flash_attention_2\"\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# input_sentences = [\n#     \"When I was young, I used to go to the park every day.\",\n#     \"We watched a new movie last week, which was very inspiring.\",\n#     \"If you had met me at that time, we would have gone out to eat.\",\n#     \"My friend has invited me to his birthday party, and I will give him a gift.\",\n# ]\n\n# batch = ip.preprocess_batch(\n#     input_sentences,\n#     src_lang=src_lang,\n#     tgt_lang=tgt_lang,\n# )\n\n# # Tokenize the sentences and generate input encodings\n# inputs = tokenizer(\n#     batch,\n#     truncation=True,\n#     padding=\"longest\",\n#     return_tensors=\"pt\",\n#     return_attention_mask=True,\n# ).to(DEVICE)\n\n# # Generate translations using the model\n# with torch.no_grad():\n#     generated_tokens = model.generate(\n#         **inputs,\n#         use_cache=True,\n#         min_length=0,\n#         max_length=256,\n#         num_beams=5,\n#         num_return_sequences=1,\n#     )\n\n# # Decode the generated tokens into text\n# generated_tokens = tokenizer.batch_decode(\n#     generated_tokens,\n#     skip_special_tokens=True,\n#     clean_up_tokenization_spaces=True,\n# )\n\n# # Postprocess the translations, including entity replacement\n# translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\n# for input_sentence, translation in zip(input_sentences, translations):\n#     print(f\"{src_lang}: {input_sentence}\")\n#     print(f\"{tgt_lang}: {translation}\")\n# end = time.time()\n# print(end - start )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T03:37:30.183342Z","iopub.execute_input":"2025-09-29T03:37:30.184248Z","iopub.status.idle":"2025-09-29T03:37:59.166349Z","shell.execute_reply.started":"2025-09-29T03:37:30.184168Z","shell.execute_reply":"2025-09-29T03:37:59.165629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n# import time\n\n# start = time.time()\n# # recommended to run this on a gpu with flash_attn installed\n# # don't set attn_implemetation if you don't have flash_attn\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-1B\"\n# # model_name = \"ai4bharat/IndicTrans3-beta\"\n# # model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name, \n#     trust_remote_code=True, \n#     torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n#     attn_implementation=\"flash_attention_2\"\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# input_sentences = [\n#     \"When I was young, I used to go to the park every day.\",\n#     \"We watched a new movie last week, which was very inspiring.\",\n#     \"If you had met me at that time, we would have gone out to eat.\",\n#     \"My friend has invited me to his birthday party, and I will give him a gift.\",\n# ]\n\n# batch = ip.preprocess_batch(\n#     input_sentences,\n#     src_lang=src_lang,\n#     tgt_lang=tgt_lang,\n# )\n\n# # Tokenize the sentences and generate input encodings\n# inputs = tokenizer(\n#     batch,\n#     truncation=True,\n#     padding=\"longest\",\n#     return_tensors=\"pt\",\n#     return_attention_mask=True,\n# ).to(DEVICE)\n\n# # Generate translations using the model\n# with torch.no_grad():\n#     generated_tokens = model.generate(\n#         **inputs,\n#         use_cache=True,\n#         min_length=0,\n#         max_length=256,\n#         num_beams=5,\n#         num_return_sequences=1,\n#     )\n\n# # Decode the generated tokens into text\n# generated_tokens = tokenizer.batch_decode(\n#     generated_tokens,\n#     skip_special_tokens=True,\n#     clean_up_tokenization_spaces=True,\n# )\n\n# # Postprocess the translations, including entity replacement\n# translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\n# for input_sentence, translation in zip(input_sentences, translations):\n#     print(f\"{src_lang}: {input_sentence}\")\n#     print(f\"{tgt_lang}: {translation}\")\n# end = time.time()\n# print(end - start )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T16:15:33.558297Z","iopub.execute_input":"2025-09-28T16:15:33.558589Z","iopub.status.idle":"2025-09-28T16:15:38.361031Z","shell.execute_reply.started":"2025-09-28T16:15:33.558565Z","shell.execute_reply":"2025-09-28T16:15:38.360118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Now Applying to our Dataset****","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit.processor import IndicProcessor\nimport json\nimport os\nfrom datasets import load_dataset\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T16:56:39.607021Z","iopub.execute_input":"2025-09-28T16:56:39.607354Z","iopub.status.idle":"2025-09-28T16:56:40.678680Z","shell.execute_reply.started":"2025-09-28T16:56:39.607328Z","shell.execute_reply":"2025-09-28T16:56:40.677879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import pickle\n# import torch\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# # from indictrans2 import IndicProcessor\n# from datasets import load_dataset\n# import re\n\n\n# # Device setup\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\n# # Load tokenizer and model\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name,\n#     trust_remote_code=True,\n#     torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n# ).to(DEVICE)\n\n# # Pre/post processor\n# ip = IndicProcessor(inference=True)\n\n# def clean_text(text):\n#     \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n#     if not text:\n#         return text\n#     # Replace sequences of 3 or more dots or dashes with a single space\n#     text = re.sub(r'[\\.\\-]{3,}', ' ', text)\n#     # Replace multiple spaces with single space\n#     text = re.sub(r'\\s+', ' ', text)\n#     # Strip leading/trailing spaces\n#     return text.strip()\n\n# def translate_batch(texts):\n#     \"\"\"Translate a batch of texts\"\"\"\n#     if not texts:\n#         return []\n\n#     # Preprocess\n#     batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n\n#     # Tokenize\n#     inputs = tokenizer(\n#         batch,\n#         truncation=True,\n#         padding=\"longest\",\n#         return_tensors=\"pt\",\n#         return_attention_mask=True,\n#     ).to(DEVICE)\n\n#     # Generate\n#     with torch.no_grad():\n#         generated_tokens = model.generate(\n#             **inputs,\n#             use_cache=True,\n#             min_length=0,\n#             max_length=256,\n#             num_beams=5,\n#             num_return_sequences=1,\n#         )\n\n#     # Decode\n#     generated_tokens = tokenizer.batch_decode(\n#         generated_tokens,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True,\n#     )\n\n#     # Postprocess\n#     translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n#     return translations\n# def translate_item(item):\n#     \"\"\"Translate only selected fields and fix choices translation, remove dots/dashes\"\"\"\n    \n#     # Translate choices separately\n#     def translate_choices(choices):\n#         \"\"\"Translate choices as proper nouns / short phrases, remove unwanted suffixes.\"\"\"\n#         translations = translate_batch(choices)\n#         cleaned = []\n#         for t in translations:\n#             for suffix in [\"‡§ï‡•ã\", \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \"‡§ï‡•ã‡•§\"]:\n#                 if t.endswith(suffix):\n#                     t = t[: -len(suffix)].strip()\n#             # Clean dots/dashes\n#             t = clean_text(t)\n#             cleaned.append(t)\n#         return cleaned\n\n#     translated_choices = translate_choices(item[\"choices\"])\n\n#     # Translate other fields in one batch\n#     texts_to_translate = [\n#         item[\"question\"] or \"\",\n#         item[\"hint\"] or \"\",\n#         item[\"lecture\"] or \"\",\n#         item[\"solution\"] or \"\",\n#         item[\"skill\"] or \"\",\n#     ]\n#     non_choice_translations = translate_batch(texts_to_translate)\n    \n#     # Clean dots/dashes for non-choice fields\n#     non_choice_translations = [clean_text(t) for t in non_choice_translations]\n\n#     # Map back\n#     idx = 0\n#     translated_item = {\n#         \"image\": item[\"image\"],\n#         \"question\": non_choice_translations[idx],\n#         \"choices\": translated_choices,\n#         \"hint\": non_choice_translations[idx + 1],\n#         \"lecture\": non_choice_translations[idx + 2],\n#         \"solution\": non_choice_translations[idx + 3],\n#         \"skill\": non_choice_translations[idx + 4],\n#         \"answer\": item[\"answer\"],\n#         \"task\": item[\"task\"],\n#         \"grade\": item[\"grade\"],\n#         \"subject\": item[\"subject\"],\n#         \"topic\": item[\"topic\"],\n#         \"category\": item[\"category\"],\n#     }\n\n#     return translated_item\n\n\n# def translate_dataset(train_data, max_items=None, output_dir=\"scienceqa_nepali\"):\n#     \"\"\"Translate dataset with progress saving\"\"\"\n#     os.makedirs(output_dir, exist_ok=True)\n\n#     total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n#     translated_items = []\n\n#     for i in range(total_items):\n#         print(f\"Translating item {i+1}/{total_items}\")\n\n#         item = dict(train_data[i])\n#         translated_item = translate_item(item)\n#         translated_items.append(translated_item)\n\n#         # Save progress every 10 items\n#         if (i + 1) % 10 == 0:\n#             with open(os.path.join(output_dir, \"progress.pkl\"), \"wb\") as f:\n#                 pickle.dump(translated_items, f)\n#             print(f\"‚úÖ Saved progress: {i+1} items\")\n\n#     # Save final results\n#     with open(os.path.join(output_dir, \"scienceqa_nepali_final.pkl\"), \"wb\") as f:\n#         pickle.dump(translated_items, f)\n\n#     with open(\n#         os.path.join(output_dir, \"scienceqa_nepali_final.json\"),\n#         \"w\",\n#         encoding=\"utf-8\",\n#     ) as f:\n#         json.dump(translated_items, f, ensure_ascii=False, indent=2, default=str)\n\n#     return translated_items\n\n\n# if __name__ == \"__main__\":\n#     # Load dataset\n#     ds = load_dataset(\"derek-thomas/ScienceQA\")\n#     train_data = ds[\"train\"]\n\n#     # Translate first 9 items\n#     translated_data = translate_dataset(train_data, max_items=9)\n\n#     print(\"üéâ Translation completed!\")\n#     print(\n#         \"Sample:\",\n#         json.dumps(translated_data[0], ensure_ascii=False, indent=2, default=str)[:500],\n#     )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:42:33.575546Z","iopub.execute_input":"2025-09-27T07:42:33.575819Z","iopub.status.idle":"2025-09-27T07:42:33.581985Z","shell.execute_reply.started":"2025-09-27T07:42:33.575801Z","shell.execute_reply":"2025-09-27T07:42:33.581159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import re\n# import pickle\n# import json\n# import torch\n# from datasets import load_dataset\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n\n# # ----------------------------\n# # CONFIG\n# # ----------------------------\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name,\n#     trust_remote_code=True,\n#     torch_dtype=torch.float16,\n#     attn_implementation=\"flash_attention_2\"  # Add this line\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# # ----------------------------\n# # UTILITY FUNCTIONS\n# # ----------------------------\n# def clean_text(text):\n#     \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n#     if not text:\n#         return text\n#     text = re.sub(r'[\\.\\-]{3,}', ' ', text)  # Replace 3+ dots/dashes with a space\n#     text = re.sub(r'\\s+', ' ', text)         # Collapse multiple spaces\n#     return text.strip()\n\n# def translate_batch(texts, num_beams=1):\n#     \"\"\"Translate a batch of texts\"\"\"\n#     if not texts:\n#         return []\n\n#     batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n#     inputs = tokenizer(\n#         batch,\n#         truncation=True,\n#         padding=\"longest\",\n#         return_tensors=\"pt\",\n#         return_attention_mask=True,\n#     ).to(DEVICE)\n\n#     with torch.no_grad():\n#         generated_tokens = model.generate(\n#             **inputs,\n#             use_cache=True,\n#             min_length=0,\n#             max_length=256,\n#             num_beams=num_beams,\n#             num_return_sequences=1,\n#         )\n\n#     decoded = tokenizer.batch_decode(\n#         generated_tokens,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True,\n#     )\n#     translations = ip.postprocess_batch(decoded, lang=tgt_lang)\n#     return [clean_text(t) for t in translations]\n\n# # ----------------------------\n# # ITEM TRANSLATION\n# # ----------------------------\n# def translate_item_texts(texts, num_choices):\n#     \"\"\"\n#     Given all texts of an item (question + choices + other fields), split back into fields.\n#     \"\"\"\n#     question = texts[0]\n#     choices = texts[1:1+num_choices]\n#     hint = texts[1+num_choices]\n#     lecture = texts[2+num_choices]\n#     solution = texts[3+num_choices]\n#     skill = texts[4+num_choices]\n\n#     # Clean unwanted suffixes in choices\n#     cleaned_choices = []\n#     suffixes = [\n#     \"‡§ï‡•ã\", \"‡§ï‡•ã‡•§\", \n#     \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \n#     \"‡§≤‡§æ‡§à\", \"‡§≤‡§æ‡§à‡•§\", \n#     \"‡§¨‡§æ‡§ü\", \"‡§¨‡§æ‡§ü‡•§\", \n#     \"‡§Æ‡§æ‡•á\", \"‡§Æ‡§æ‡•á‡•§\",\n#     \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\", \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø‡•§\",\n#     \"‡§∏‡§Å‡§ó\", \"‡§∏‡§Å‡§ó‡•§\"\n# ]\n#     for c in choices:\n#         for suffix in suffixes:\n#             if c.endswith(suffix):\n#                 c = c[:-len(suffix)].strip()\n#         cleaned_choices.append(clean_text(c))\n\n#     return question, cleaned_choices, hint, lecture, solution, skill\n\n# # ----------------------------\n# # DATASET TRANSLATION\n# # ----------------------------\n# def translate_dataset(train_data, max_items=None, batch_size=5, output_dir=\"scienceqa_nepali\"):\n#     os.makedirs(output_dir, exist_ok=True)\n#     total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n#     translated_items = []\n\n#     for start_idx in range(0, total_items, batch_size):\n#         # Correct batch slicing\n#         batch = [train_data[i] for i in range(start_idx, min(start_idx+batch_size, total_items))]\n\n#         texts_per_item = []\n#         counts_per_item = []\n\n#         for item in batch:\n#             item_texts = [item['question']] + item['choices'] + [\n#                 item['hint'] or \"\",\n#                 item['lecture'] or \"\",\n#                 item['solution'] or \"\",\n#                 item['skill'] or \"\"\n#             ]\n#             texts_per_item.extend(item_texts)\n#             counts_per_item.append(len(item_texts))\n\n#         # Translate entire batch\n#         translations = translate_batch(texts_per_item, num_beams=3)\n\n#         # Map translations back to items\n#         idx = 0\n#         for item, count in zip(batch, counts_per_item):\n#             item_translations = translations[idx:idx+count]\n#             idx += count\n\n#             question, choices, hint, lecture, solution, skill = translate_item_texts(item_translations, len(item['choices']))\n\n#             translated_item = {\n#                 \"image\": item[\"image\"],\n#                 \"question\": question,\n#                 \"choices\": choices,\n#                 \"hint\": hint,\n#                 \"lecture\": lecture,\n#                 \"solution\": solution,\n#                 \"skill\": skill,\n#                 \"answer\": item[\"answer\"],\n#                 \"task\": item[\"task\"],\n#                 \"grade\": item[\"grade\"],\n#                 \"subject\": item[\"subject\"],\n#                 \"topic\": item[\"topic\"],\n#                 \"category\": item[\"category\"],\n#             }\n#             translated_items.append(translated_item)\n\n#         # Save progress after each batch\n#         with open(os.path.join(output_dir, \"progress.pkl\"), 'wb') as f:\n#             pickle.dump(translated_items, f)\n#         print(f\"Translated {min(start_idx+batch_size, total_items)}/{total_items} items\")\n\n#     # Save final results\n#     with open(os.path.join(output_dir, \"scienceqa_nepali_final.pkl\"), 'wb') as f:\n#         pickle.dump(translated_items, f)\n#     with open(os.path.join(output_dir, \"scienceqa_nepali_final.json\"), 'w', encoding='utf-8') as f:\n#         json.dump(translated_items, f, ensure_ascii=False, indent=2, default=str)\n\n#     return translated_items\n\n# # ----------------------------\n# # MAIN\n# # ----------------------------\n# if __name__ == \"__main__\":\n#     ds = load_dataset(\"derek-thomas/ScienceQA\")\n#     train_data = ds['train']\n\n#     # Translate first 20 items for testing\n#     translated_data = translate_dataset(train_data, max_items=1, batch_size=1)\n#     # batch size jati badhayo teti ramro\n\n#     print(\"Translation completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T17:11:17.118718Z","iopub.execute_input":"2025-09-28T17:11:17.119479Z","iopub.status.idle":"2025-09-28T17:12:20.368281Z","shell.execute_reply.started":"2025-09-28T17:11:17.119447Z","shell.execute_reply":"2025-09-28T17:12:20.367273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[:7]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T16:17:48.687743Z","iopub.execute_input":"2025-09-28T16:17:48.688092Z","iopub.status.idle":"2025-09-28T16:17:48.706935Z","shell.execute_reply.started":"2025-09-28T16:17:48.688071Z","shell.execute_reply":"2025-09-28T16:17:48.706359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# translated_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T16:18:09.910712Z","iopub.execute_input":"2025-09-28T16:18:09.911008Z","iopub.status.idle":"2025-09-28T16:18:09.917495Z","shell.execute_reply.started":"2025-09-28T16:18:09.910986Z","shell.execute_reply":"2025-09-28T16:18:09.916794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# translated_data[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:40:20.571559Z","iopub.execute_input":"2025-09-27T07:40:20.571808Z","iopub.status.idle":"2025-09-27T07:40:20.576919Z","shell.execute_reply.started":"2025-09-27T07:40:20.571791Z","shell.execute_reply":"2025-09-27T07:40:20.576210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T07:38:40.503884Z","iopub.execute_input":"2025-09-27T07:38:40.504468Z","iopub.status.idle":"2025-09-27T07:38:40.512758Z","shell.execute_reply.started":"2025-09-27T07:38:40.504445Z","shell.execute_reply":"2025-09-27T07:38:40.512063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import re\n# import pickle\n# import json\n# import torch\n# from datasets import load_dataset\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n\n# # ----------------------------\n# # CONFIG\n# # ----------------------------\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name,\n#     trust_remote_code=True,\n#     torch_dtype=torch.float16,\n#     attn_implementation=\"flash_attention_2\"  # Add this line\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# # ----------------------------\n# # UTILITY FUNCTIONS\n# # ----------------------------\n# def clean_text(text):\n#     \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n#     if not text:\n#         return text\n#     text = re.sub(r'[\\.\\-]{3,}', ' ', text)  # Replace 3+ dots/dashes with a space\n#     text = re.sub(r'\\s+', ' ', text)         # Collapse multiple spaces\n#     return text.strip()\n\n# def translate_batch(texts, num_beams=1):\n#     \"\"\"Translate a batch of texts\"\"\"\n#     if not texts:\n#         return []\n\n#     batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n#     inputs = tokenizer(\n#         batch,\n#         truncation=True,\n#         padding=\"longest\",\n#         return_tensors=\"pt\",\n#         return_attention_mask=True,\n#     ).to(DEVICE)\n\n#     with torch.no_grad():\n#         generated_tokens = model.generate(\n#             **inputs,\n#             use_cache=True,\n#             min_length=0,\n#             max_length=256,\n#             num_beams=num_beams,\n#             num_return_sequences=1,\n#         )\n\n#     decoded = tokenizer.batch_decode(\n#         generated_tokens,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True,\n#     )\n#     translations = ip.postprocess_batch(decoded, lang=tgt_lang)\n#     return [clean_text(t) for t in translations]\n\n# # ----------------------------\n# # ITEM TRANSLATION\n# # ----------------------------\n# def translate_item_texts(texts, num_choices):\n#     \"\"\"\n#     Given all texts of an item (question + choices + other fields), split back into fields.\n#     \"\"\"\n#     question = texts[0]\n#     choices = texts[1:1+num_choices]\n#     hint = texts[1+num_choices]\n#     lecture = texts[2+num_choices]\n#     solution = texts[3+num_choices]\n#     skill = texts[4+num_choices]\n\n#     # Clean unwanted suffixes in choices\n#     cleaned_choices = []\n#     suffixes = [\n#     \"‡§ï‡•ã\", \"‡§ï‡•ã‡•§\", \n#     \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \n#     \"‡§≤‡§æ‡§à\", \"‡§≤‡§æ‡§à‡•§\", \n#     \"‡§¨‡§æ‡§ü\", \"‡§¨‡§æ‡§ü‡•§\", \n#     \"‡§Æ‡§æ‡•á\", \"‡§Æ‡§æ‡•á‡•§\",\n#     \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\", \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø‡•§\",\n#     \"‡§∏‡§Å‡§ó\", \"‡§∏‡§Å‡§ó‡•§\"\n# ]\n#     for c in choices:\n#         for suffix in suffixes:\n#             if c.endswith(suffix):\n#                 c = c[:-len(suffix)].strip()\n#         cleaned_choices.append(clean_text(c))\n\n#     return question, cleaned_choices, hint, lecture, solution, skill\n\n# # ----------------------------\n# # DATASET TRANSLATION\n# # ----------------------------\n# def translate_dataset(train_data, max_items=None, batch_size=5, output_dir=\"scienceqa_nepali\", accumulate=True):\n#     os.makedirs(output_dir, exist_ok=True)\n    \n#     # Load existing translations if they exist\n#     final_file = os.path.join(output_dir, \"scienceqa_nepali_final.json\")\n#     if accumulate and os.path.exists(final_file):\n#         with open(final_file, 'r', encoding='utf-8') as f:\n#             existing_items = json.load(f)\n#         print(f\"Loaded {len(existing_items)} existing translations\")\n#     else:\n#         existing_items = []\n    \n#     total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n#     translated_items = []\n\n#     for start_idx in range(0, total_items, batch_size):\n#         # Get batch items as dictionaries\n#         batch = []\n#         for i in range(start_idx, min(start_idx+batch_size, total_items)):\n#             batch.append(dict(train_data[i]))\n\n#         texts_per_item = []\n#         counts_per_item = []\n\n#         for item in batch:\n#             item_texts = [item['question']] + item['choices'] + [\n#                 item.get('hint', '') or \"\",\n#                 item.get('lecture', '') or \"\",\n#                 item.get('solution', '') or \"\",\n#                 item.get('skill', '') or \"\"\n#             ]\n#             texts_per_item.extend(item_texts)\n#             counts_per_item.append(len(item_texts))\n\n#         # Translate entire batch\n#         translations = translate_batch(texts_per_item, num_beams=3)\n\n#         # Map translations back to items\n#         idx = 0\n#         for item, count in zip(batch, counts_per_item):\n#             item_translations = translations[idx:idx+count]\n#             idx += count\n\n#             question, choices, hint, lecture, solution, skill = translate_item_texts(item_translations, len(item['choices']))\n\n#             translated_item = {\n#                 \"image\": item[\"image\"],\n#                 \"question\": question,\n#                 \"choices\": choices,\n#                 \"hint\": hint,\n#                 \"lecture\": lecture,\n#                 \"solution\": solution,\n#                 \"skill\": skill,\n#                 \"answer\": item[\"answer\"],\n#                 \"task\": item[\"task\"],\n#                 \"grade\": item[\"grade\"],\n#                 \"subject\": item[\"subject\"],\n#                 \"topic\": item[\"topic\"],\n#                 \"category\": item[\"category\"],\n#             }\n#             translated_items.append(translated_item)\n\n#         # Save progress after each batch (only current batch)\n#         with open(os.path.join(output_dir, \"progress.pkl\"), 'wb') as f:\n#             pickle.dump(translated_items, f)\n#         print(f\"Translated {min(start_idx+batch_size, total_items)}/{total_items} items\")\n\n#     # Combine existing + new translations\n#     all_items = existing_items + translated_items\n    \n#     # Save final results (accumulated)\n#     with open(os.path.join(output_dir, \"scienceqa_nepali_final.pkl\"), 'wb') as f:\n#         pickle.dump(all_items, f)\n#     with open(final_file, 'w', encoding='utf-8') as f:\n#         json.dump(all_items, f, ensure_ascii=False, indent=2, default=str)\n    \n#     print(f\"Translation completed! Total items now: {len(all_items)} (added {len(translated_items)} new items)\")\n#     return translated_items\n# # ----------------------------\n# # MAIN\n# # ----------------------------\n# if __name__ == \"__main__\":\n#     ds = load_dataset(\"derek-thomas/ScienceQA\")\n#     train_data = ds['train']\n\n#     # Translate first 20 items for testing\n#     selected_data = train_data.select(range(0, len(train_data)))\n#     translated_data = translate_dataset(selected_data, max_items=300, batch_size=10)\n#     # batch size jati badhayo teti ramro\n\n#     print(\"Translation completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T03:38:36.927853Z","iopub.execute_input":"2025-09-29T03:38:36.928432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:34:19.078855Z","iopub.execute_input":"2025-09-29T04:34:19.079541Z","iopub.status.idle":"2025-09-29T04:34:19.085795Z","shell.execute_reply.started":"2025-09-29T04:34:19.079500Z","shell.execute_reply":"2025-09-29T04:34:19.084991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open(\"data.json\", \"w\") as file:\n#     json.dump(translated_data, file, indent=4) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:43:45.494933Z","iopub.execute_input":"2025-09-29T04:43:45.495225Z","iopub.status.idle":"2025-09-29T04:43:45.518962Z","shell.execute_reply.started":"2025-09-29T04:43:45.495204Z","shell.execute_reply":"2025-09-29T04:43:45.518052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[998]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:37:44.813978Z","iopub.execute_input":"2025-09-29T04:37:44.814347Z","iopub.status.idle":"2025-09-29T04:37:44.820984Z","shell.execute_reply.started":"2025-09-29T04:37:44.814315Z","shell.execute_reply":"2025-09-29T04:37:44.820370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[104]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T17:05:11.099686Z","iopub.execute_input":"2025-09-28T17:05:11.099962Z","iopub.status.idle":"2025-09-28T17:05:11.105713Z","shell.execute_reply.started":"2025-09-28T17:05:11.099941Z","shell.execute_reply":"2025-09-28T17:05:11.104946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T05:54:20.694344Z","iopub.execute_input":"2025-09-29T05:54:20.695220Z","iopub.status.idle":"2025-09-29T05:54:20.699779Z","shell.execute_reply.started":"2025-09-29T05:54:20.695178Z","shell.execute_reply":"2025-09-29T05:54:20.699140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import re\n# import pickle\n# import json\n# import torch\n# from datasets import load_dataset\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n\n# # ----------------------------\n# # CONFIG\n# # ----------------------------\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name,\n#     trust_remote_code=True,\n#     torch_dtype=torch.float16,\n#     attn_implementation=\"flash_attention_2\"\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# # ----------------------------\n# # UTILITY FUNCTIONS\n# # ----------------------------\n# def clean_text(text):\n#     \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n#     if not text:\n#         return text\n#     text = re.sub(r'[\\.\\-]{3,}', ' ', text)\n#     text = re.sub(r'\\s+', ' ', text)\n#     return text.strip()\n\n# def translate_batch(texts, num_beams=1):\n#     \"\"\"Translate a batch of texts\"\"\"\n#     if not texts:\n#         return []\n\n#     batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n#     inputs = tokenizer(\n#         batch,\n#         truncation=True,\n#         padding=\"longest\",\n#         return_tensors=\"pt\",\n#         return_attention_mask=True,\n#     ).to(DEVICE)\n\n#     with torch.no_grad():\n#         generated_tokens = model.generate(\n#             **inputs,\n#             use_cache=True,\n#             min_length=0,\n#             max_length=256,\n#             num_beams=num_beams,\n#             num_return_sequences=1,\n#         )\n\n#     decoded = tokenizer.batch_decode(\n#         generated_tokens,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True,\n#     )\n#     translations = ip.postprocess_batch(decoded, lang=tgt_lang)\n#     return [clean_text(t) for t in translations]\n\n# def save_image(image, image_id, image_dir=\"scienceqa_images\"):\n#     \"\"\"Save PIL Image and return its path\"\"\"\n#     os.makedirs(image_dir, exist_ok=True)\n    \n#     if image is None:\n#         return None\n    \n#     image_path = os.path.join(image_dir, f\"{image_id}.png\")\n#     image.save(image_path)\n#     return image_path\n\n# # ----------------------------\n# # ITEM TRANSLATION\n# # ----------------------------\n# def translate_item_texts(texts, num_choices):\n#     \"\"\"\n#     Given all texts of an item (question + choices + other fields), split back into fields.\n#     \"\"\"\n#     question = texts[0]\n#     choices = texts[1:1+num_choices]\n#     hint = texts[1+num_choices]\n#     lecture = texts[2+num_choices]\n#     solution = texts[3+num_choices]\n#     skill = texts[4+num_choices]\n\n#     # Clean unwanted suffixes in choices\n#     cleaned_choices = []\n#     suffixes = [\n#     \"‡§ï‡•ã\", \"‡§ï‡•ã‡•§\", \n#     \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \n#     \"‡§≤‡§æ‡§à\", \"‡§≤‡§æ‡§à‡•§\", \n#     \"‡§¨‡§æ‡§ü\", \"‡§¨‡§æ‡§ü‡•§\", \n#     \"‡§Æ‡§æ‡•á\", \"‡§Æ‡§æ‡•á‡•§\",\n#     \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\", \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø‡•§\",\n#     \"‡§∏‡§Å‡§ó\", \"‡§∏‡§Å‡§ó‡•§\"\n# ]\n#     for c in choices:\n#         for suffix in suffixes:\n#             if c.endswith(suffix):\n#                 c = c[:-len(suffix)].strip()\n#         cleaned_choices.append(clean_text(c))\n\n#     return question, cleaned_choices, hint, lecture, solution, skill\n\n# # ----------------------------\n# # DATASET TRANSLATION\n# # ----------------------------\n# def translate_dataset(train_data, max_items=None, batch_size=5, output_dir=\"scienceqa_nepali\", accumulate=True):\n#     os.makedirs(output_dir, exist_ok=True)\n    \n#     # Load existing translations if they exist\n#     final_file = os.path.join(output_dir, \"scienceqa_nepali_final.json\")\n#     if accumulate and os.path.exists(final_file):\n#         with open(final_file, 'r', encoding='utf-8') as f:\n#             existing_items = json.load(f)\n#         print(f\"Loaded {len(existing_items)} existing translations\")\n#     else:\n#         existing_items = []\n    \n#     total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n#     translated_items = []\n\n#     for start_idx in range(0, total_items, batch_size):\n#         # Get batch items as dictionaries\n#         batch = []\n#         for i in range(start_idx, min(start_idx+batch_size, total_items)):\n#             batch.append(dict(train_data[i]))\n\n#         texts_per_item = []\n#         counts_per_item = []\n\n#         for item in batch:\n#             item_texts = [item['question']] + item['choices'] + [\n#                 item.get('hint', '') or \"\",\n#                 item.get('lecture', '') or \"\",\n#                 item.get('solution', '') or \"\",\n#                 item.get('skill', '') or \"\"\n#             ]\n#             texts_per_item.extend(item_texts)\n#             counts_per_item.append(len(item_texts))\n\n#         # Translate entire batch\n#         translations = translate_batch(texts_per_item, num_beams=3)\n\n#         # Map translations back to items\n#         idx = 0\n#         for item_idx, (item, count) in enumerate(zip(batch, counts_per_item)):\n#             item_translations = translations[idx:idx+count]\n#             idx += count\n\n#             question, choices, hint, lecture, solution, skill = translate_item_texts(item_translations, len(item['choices']))\n\n#             # Save image and get path\n#             image_path = save_image(item[\"image\"], start_idx + item_idx)\n\n#             translated_item = {\n#                 \"image\": image_path,\n#                 \"question\": question,\n#                 \"choices\": choices,\n#                 \"hint\": hint,\n#                 \"lecture\": lecture,\n#                 \"solution\": solution,\n#                 \"skill\": skill,\n#                 \"answer\": item[\"answer\"],\n#                 \"task\": item[\"task\"],\n#                 \"grade\": item[\"grade\"],\n#                 \"subject\": item[\"subject\"],\n#                 \"topic\": item[\"topic\"],\n#                 \"category\": item[\"category\"],\n#             }\n#             translated_items.append(translated_item)\n\n#         # Save progress after each batch (only current batch)\n#         with open(os.path.join(output_dir, \"progress.pkl\"), 'wb') as f:\n#             pickle.dump(translated_items, f)\n#         print(f\"Translated {min(start_idx+batch_size, total_items)}/{total_items} items\")\n\n#     # Combine existing + new translations\n#     all_items = existing_items + translated_items\n    \n#     # Save final results (accumulated)\n#     with open(os.path.join(output_dir, \"scienceqa_nepali_final.pkl\"), 'wb') as f:\n#         pickle.dump(all_items, f)\n#     with open(final_file, 'w', encoding='utf-8') as f:\n#         json.dump(all_items, f, ensure_ascii=False, indent=2, default=str)\n    \n#     print(f\"Translation completed! Total items now: {len(all_items)} (added {len(translated_items)} new items)\")\n#     return translated_items\n\n# # ----------------------------\n# # MAIN\n# # ----------------------------\n# if __name__ == \"__main__\":\n#     ds = load_dataset(\"derek-thomas/ScienceQA\")\n#     train_data = ds['train']\n\n#     # Translate first 20 items for testing\n#     selected_data = train_data.select(range(0, len(train_data)))\n#     translated_data = translate_dataset(selected_data, max_items=2, batch_size=2)\n#         # Zip the images folder\n#     shutil.make_archive('scienceqa_images_backup', 'zip', 'scienceqa_images')\n#     print(\"‚úÖ Images zipped! Download 'scienceqa_images_backup.zip' from Output sidebar\")\n    \n#     # Also zip the translation results\n#     shutil.make_archive('scienceqa_nepali_backup', 'zip', 'scienceqa_nepali')\n#     print(\"‚úÖ Translations zipped! Download 'scienceqa_nepali_backup.zip' from Output sidebar\")\n\n#     print(\"Translation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T05:54:28.228334Z","iopub.execute_input":"2025-09-29T05:54:28.228899Z","iopub.status.idle":"2025-09-29T05:54:41.867391Z","shell.execute_reply.started":"2025-09-29T05:54:28.228876Z","shell.execute_reply":"2025-09-29T05:54:41.866477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:38:25.315845Z","iopub.execute_input":"2025-09-29T04:38:25.316477Z","iopub.status.idle":"2025-09-29T04:38:25.340302Z","shell.execute_reply.started":"2025-09-29T04:38:25.316443Z","shell.execute_reply":"2025-09-29T04:38:25.339389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T05:57:43.242935Z","iopub.execute_input":"2025-09-29T05:57:43.243287Z","iopub.status.idle":"2025-09-29T05:57:43.252477Z","shell.execute_reply.started":"2025-09-29T05:57:43.243261Z","shell.execute_reply":"2025-09-29T05:57:43.251801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport torch\nfrom datasets import load_dataset, Dataset, Features, Image, Value, Sequence\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit.processor import IndicProcessor\n\n# ----------------------------\n# CONFIG\n# ----------------------------\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsrc_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\nmodel_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    attn_implementation=\"flash_attention_2\"\n).to(DEVICE)\n\nip = IndicProcessor(inference=True)\n\n# ----------------------------\n# UTILITY FUNCTIONS\n# ----------------------------\ndef clean_text(text):\n    \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n    if not text:\n        return text\n    text = re.sub(r'[\\.\\-]{3,}', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef translate_batch(texts, num_beams=1):\n    \"\"\"Translate a batch of texts\"\"\"\n    if not texts:\n        return []\n\n    batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n    inputs = tokenizer(\n        batch,\n        truncation=True,\n        padding=\"longest\",\n        return_tensors=\"pt\",\n        return_attention_mask=True,\n    ).to(DEVICE)\n\n    with torch.no_grad():\n        generated_tokens = model.generate(\n            **inputs,\n            use_cache=True,\n            min_length=0,\n            max_length=256,\n            num_beams=num_beams,\n            num_return_sequences=1,\n        )\n\n    decoded = tokenizer.batch_decode(\n        generated_tokens,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True,\n    )\n    translations = ip.postprocess_batch(decoded, lang=tgt_lang)\n    return [clean_text(t) for t in translations]\n\n# ----------------------------\n# ITEM TRANSLATION\n# ----------------------------\ndef translate_item_texts(texts, num_choices):\n    \"\"\"\n    Given all texts of an item (question + choices + other fields), split back into fields.\n    \"\"\"\n    question = texts[0]\n    choices = texts[1:1+num_choices]\n    hint = texts[1+num_choices]\n    lecture = texts[2+num_choices]\n    solution = texts[3+num_choices]\n    skill = texts[4+num_choices]\n\n    # Clean unwanted suffixes in choices\n    cleaned_choices = []\n    suffixes = [\n    \"‡§ï‡•ã\", \"‡§ï‡•ã‡•§\", \n    \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \n    \"‡§≤‡§æ‡§à\", \"‡§≤‡§æ‡§à‡•§\", \n    \"‡§¨‡§æ‡§ü\", \"‡§¨‡§æ‡§ü‡•§\", \n    \"‡§Æ‡§æ‡•á\", \"‡§Æ‡§æ‡•á‡•§\",\n    \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\", \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø‡•§\",\n    \"‡§∏‡§Å‡§ó\", \"‡§∏‡§Å‡§ó‡•§\"\n]\n    for c in choices:\n        for suffix in suffixes:\n            if c.endswith(suffix):\n                c = c[:-len(suffix)].strip()\n        cleaned_choices.append(clean_text(c))\n\n    return question, cleaned_choices, hint, lecture, solution, skill\n\ndef create_huggingface_dataset(translated_items):\n    \"\"\"Convert translated items to HuggingFace Dataset\"\"\"\n    \n    features = Features({\n        'image': Image(),\n        'question': Value('string'),\n        'choices': Sequence(Value('string')),\n        'hint': Value('string'),\n        'lecture': Value('string'),\n        'solution': Value('string'),\n        'skill': Value('string'),\n        'answer': Value('int32'),\n        'task': Value('string'),\n        'grade': Value('string'),\n        'subject': Value('string'),\n        'topic': Value('string'),\n        'category': Value('string'),\n    })\n    \n    dataset = Dataset.from_dict(\n        {k: [item[k] for item in translated_items] for k in translated_items[0].keys()},\n        features=features\n    )\n    \n    return dataset\n\n# ----------------------------\n# DATASET TRANSLATION\n# ----------------------------\ndef translate_dataset(train_data, max_items=None, batch_size=5, output_dir=\"scienceqa_nepali\", accumulate=True):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load existing translations if they exist\n    dataset_path = os.path.join(output_dir, \"scienceqa_nepali_dataset\")\n    if accumulate and os.path.exists(dataset_path):\n        from datasets import load_from_disk\n        existing_dataset = load_from_disk(dataset_path)\n        existing_items = list(existing_dataset)\n        print(f\"Loaded {len(existing_items)} existing translations from dataset\")\n    else:\n        existing_items = []\n    \n    total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n    translated_items = []\n\n    for start_idx in range(0, total_items, batch_size):\n        # Get batch items as dictionaries\n        batch = []\n        for i in range(start_idx, min(start_idx+batch_size, total_items)):\n            batch.append(dict(train_data[i]))\n\n        texts_per_item = []\n        counts_per_item = []\n\n        for item in batch:\n            item_texts = [item['question']] + item['choices'] + [\n                item.get('hint', '') or \"\",\n                item.get('lecture', '') or \"\",\n                item.get('solution', '') or \"\",\n                item.get('skill', '') or \"\"\n            ]\n            texts_per_item.extend(item_texts)\n            counts_per_item.append(len(item_texts))\n\n        # Translate entire batch\n        translations = translate_batch(texts_per_item, num_beams=3)\n\n        # Map translations back to items\n        idx = 0\n        for item, count in zip(batch, counts_per_item):\n            item_translations = translations[idx:idx+count]\n            idx += count\n\n            question, choices, hint, lecture, solution, skill = translate_item_texts(item_translations, len(item['choices']))\n\n            translated_item = {\n                \"image\": item[\"image\"],\n                \"question\": question,\n                \"choices\": choices,\n                \"hint\": hint,\n                \"lecture\": lecture,\n                \"solution\": solution,\n                \"skill\": skill,\n                \"answer\": item[\"answer\"],\n                \"task\": item[\"task\"],\n                \"grade\": item[\"grade\"],\n                \"subject\": item[\"subject\"],\n                \"topic\": item[\"topic\"],\n                \"category\": item[\"category\"],\n            }\n            translated_items.append(translated_item)\n\n        # Save progress after each batch (only current batch)\n        with open(os.path.join(output_dir, \"progress.pkl\"), 'wb') as f:\n            pickle.dump(translated_items, f)\n        print(f\"Translated {min(start_idx+batch_size, total_items)}/{total_items} items\")\n\n    # Combine existing + new translations\n    all_items = existing_items + translated_items\n    \n    # Save as HuggingFace Dataset\n    hf_dataset = create_huggingface_dataset(all_items)\n    hf_dataset.save_to_disk(os.path.join(output_dir, \"scienceqa_nepali_dataset\"))\n    \n    print(f\"Translation completed! Total items: {len(all_items)} (added {len(translated_items)} new items)\")\n    print(f\"Dataset saved to: {output_dir}/scienceqa_nepali_dataset\")\n    return translated_items\n\n# ----------------------------\n# MAIN\n# ----------------------------\nif __name__ == \"__main__\":\n    ds = load_dataset(\"derek-thomas/ScienceQA\")\n    train_data = ds['train']\n\n    selected_data = train_data.select(range(7000, len(train_data)))\n    translated_data = translate_dataset(selected_data, max_items=1000, batch_size=10)\n\n    print(\"Translation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T07:25:38.856300Z","iopub.execute_input":"2025-10-06T07:25:38.856896Z","iopub.status.idle":"2025-10-06T07:26:32.548256Z","shell.execute_reply.started":"2025-10-06T07:25:38.856866Z","shell.execute_reply":"2025-10-06T07:26:32.547582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# translated_data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T07:27:14.730530Z","iopub.execute_input":"2025-10-06T07:27:14.731138Z","iopub.status.idle":"2025-10-06T07:27:14.737030Z","shell.execute_reply.started":"2025-10-06T07:27:14.731109Z","shell.execute_reply":"2025-10-06T07:27:14.736304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# After your translation completes, add this:\nshutil.make_archive('scienceqa_nepali_dataset', 'zip', 'scienceqa_nepali/scienceqa_nepali_dataset')\nprint(\"‚úÖ Dataset zipped! Download 'scienceqa_nepali_dataset.zip' from Output sidebar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:12:55.976815Z","iopub.execute_input":"2025-09-29T10:12:55.977280Z","iopub.status.idle":"2025-09-29T10:13:00.088929Z","shell.execute_reply.started":"2025-09-29T10:12:55.977255Z","shell.execute_reply":"2025-09-29T10:13:00.088262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check what's in your current directory\n# print(os.listdir('.'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T06:41:09.211055Z","iopub.execute_input":"2025-09-29T06:41:09.211617Z","iopub.status.idle":"2025-09-29T06:41:09.216147Z","shell.execute_reply.started":"2025-09-29T06:41:09.211579Z","shell.execute_reply":"2025-09-29T06:41:09.215375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import Dataset\n# # Path inside Kaggle\n# data_path = \"/kaggle/input/scienceqa-dataset/data-00000-of-00001.arrow\"\n# transdataset = Dataset.from_file(data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T11:58:24.753261Z","iopub.execute_input":"2025-09-30T11:58:24.754065Z","iopub.status.idle":"2025-09-30T11:58:24.989247Z","shell.execute_reply.started":"2025-09-30T11:58:24.754041Z","shell.execute_reply":"2025-09-30T11:58:24.988710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(transdataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T11:58:47.589237Z","iopub.execute_input":"2025-09-30T11:58:47.589559Z","iopub.status.idle":"2025-09-30T11:58:47.594095Z","shell.execute_reply.started":"2025-09-30T11:58:47.589538Z","shell.execute_reply":"2025-09-30T11:58:47.593439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import load_from_disk\n\n# # Load the dataset\n# dataset = load_from_disk(\"/kaggle/input/scienceqa-dataset\") \n# print(f\"Total items: {len(dataset)}\")\n# print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:33:29.716058Z","iopub.execute_input":"2025-09-30T12:33:29.716342Z","iopub.status.idle":"2025-09-30T12:33:29.730194Z","shell.execute_reply.started":"2025-09-30T12:33:29.716321Z","shell.execute_reply":"2025-09-30T12:33:29.729597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import load_from_disk\n\n# # Load the dataset\n# dataset = load_from_disk(\"scienceqa_nepali/scienceqa_nepali_dataset\")  # or wherever you extracted it\n\n# print(f\"Total items: {len(dataset)}\")\n# print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:13:23.895408Z","iopub.execute_input":"2025-09-29T10:13:23.895667Z","iopub.status.idle":"2025-09-29T10:13:23.907265Z","shell.execute_reply.started":"2025-09-29T10:13:23.895648Z","shell.execute_reply":"2025-09-29T10:13:23.906676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset[2999]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:13:35.042640Z","iopub.execute_input":"2025-09-29T10:13:35.043366Z","iopub.status.idle":"2025-09-29T10:13:35.048861Z","shell.execute_reply.started":"2025-09-29T10:13:35.043326Z","shell.execute_reply":"2025-09-29T10:13:35.048267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[799]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:14:45.968435Z","iopub.execute_input":"2025-09-29T07:14:45.968709Z","iopub.status.idle":"2025-09-29T07:14:45.974625Z","shell.execute_reply.started":"2025-09-29T07:14:45.968685Z","shell.execute_reply":"2025-09-29T07:14:45.974004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import re\n# import pickle\n# import torch\n# from datasets import load_dataset, Dataset, Features, Image, Value, Sequence\n# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# from IndicTransToolkit.processor import IndicProcessor\n\n# # ----------------------------\n# # CONFIG\n# # ----------------------------\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# src_lang, tgt_lang = \"eng_Latn\", \"npi_Deva\"\n# model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     model_name,\n#     trust_remote_code=True,\n#     torch_dtype=torch.float16,\n#     attn_implementation=\"flash_attention_2\"\n# ).to(DEVICE)\n\n# ip = IndicProcessor(inference=True)\n\n# # ----------------------------\n# # UTILITY FUNCTIONS\n# # ----------------------------\n# def clean_text(text):\n#     \"\"\"Remove long sequences of dots or dashes and trim spaces.\"\"\"\n#     if not text:\n#         return text\n#     text = re.sub(r'[\\.\\-]{3,}', ' ', text)\n#     text = re.sub(r'\\s+', ' ', text)\n#     return text.strip()\n\n# def translate_batch(texts, num_beams=1):\n#     \"\"\"Translate a batch of texts\"\"\"\n#     if not texts:\n#         return []\n\n#     batch = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n#     inputs = tokenizer(\n#         batch,\n#         truncation=True,\n#         padding=\"longest\",\n#         return_tensors=\"pt\",\n#         return_attention_mask=True,\n#     ).to(DEVICE)\n\n#     with torch.no_grad():\n#         generated_tokens = model.generate(\n#             **inputs,\n#             use_cache=True,\n#             min_length=0,\n#             max_length=256,\n#             num_beams=num_beams,\n#             num_return_sequences=1,\n#         )\n\n#     decoded = tokenizer.batch_decode(\n#         generated_tokens,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True,\n#     )\n#     translations = ip.postprocess_batch(decoded, lang=tgt_lang)\n#     return [clean_text(t) for t in translations]\n\n# # ----------------------------\n# # ITEM TRANSLATION\n# # ----------------------------\n# def translate_item_texts(texts, num_choices):\n#     \"\"\"\n#     Given all texts of an item (question + choices + other fields), split back into fields.\n#     \"\"\"\n#     question = texts[0]\n#     choices = texts[1:1+num_choices]\n#     hint = texts[1+num_choices]\n#     lecture = texts[2+num_choices]\n#     solution = texts[3+num_choices]\n#     skill = texts[4+num_choices]\n\n#     # Clean unwanted suffixes in choices\n#     cleaned_choices = []\n#     suffixes = [\n#     \"‡§ï‡•ã\", \"‡§ï‡•ã‡•§\", \n#     \"‡§Æ‡§æ\", \"‡§Æ‡§æ‡•§\", \n#     \"‡§≤‡§æ‡§à\", \"‡§≤‡§æ‡§à‡•§\", \n#     \"‡§¨‡§æ‡§ü\", \"‡§¨‡§æ‡§ü‡•§\", \n#     \"‡§Æ‡§æ‡•á\", \"‡§Æ‡§æ‡•á‡•§\",\n#     \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\", \"‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø‡•§\",\n#     \"‡§∏‡§Å‡§ó\", \"‡§∏‡§Å‡§ó‡•§\"\n# ]\n#     for c in choices:\n#         for suffix in suffixes:\n#             if c.endswith(suffix):\n#                 c = c[:-len(suffix)].strip()\n#         cleaned_choices.append(clean_text(c))\n\n#     return question, cleaned_choices, hint, lecture, solution, skill\n\n# def create_huggingface_dataset(translated_items):\n#     \"\"\"Convert translated items to HuggingFace Dataset\"\"\"\n    \n#     features = Features({\n#         'image': Image(),\n#         'question': Value('string'),\n#         'choices': Sequence(Value('string')),\n#         'hint': Value('string'),\n#         'lecture': Value('string'),\n#         'solution': Value('string'),\n#         'skill': Value('string'),\n#         'answer': Value('int32'),\n#         'task': Value('string'),\n#         'grade': Value('string'),\n#         'subject': Value('string'),\n#         'topic': Value('string'),\n#         'category': Value('string'),\n#     })\n    \n#     dataset = Dataset.from_dict(\n#         {k: [item[k] for item in translated_items] for k in translated_items[0].keys()},\n#         features=features\n#     )\n    \n#     return dataset\n\n# # ----------------------------\n# # DATASET TRANSLATION\n# # ----------------------------\n# def translate_dataset(train_data, max_items=None, batch_size=5, output_dir=\"scienceqa_nepali\", accumulate=True):\n#     os.makedirs(output_dir, exist_ok=True)\n    \n#     # Load existing translations if they exist\n#     dataset_path = os.path.join(output_dir, \"scienceqa_nepali_dataset\")\n#     if accumulate and os.path.exists(dataset_path):\n#         from datasets import load_from_disk\n#         existing_dataset = load_from_disk(dataset_path)\n#         existing_items = list(existing_dataset)\n#         print(f\"Loaded {len(existing_items)} existing translations from dataset\")\n#     else:\n#         existing_items = []\n\n    \n#     total_items = min(max_items, len(train_data)) if max_items else len(train_data)\n#     translated_items = []\n\n#     for start_idx in range(0, total_items, batch_size):\n#         # Get batch items as dictionaries\n#         batch = []\n#         for i in range(start_idx, min(start_idx+batch_size, total_items)):\n#             batch.append(dict(train_data[i]))\n\n#         texts_per_item = []\n#         counts_per_item = []\n\n#         for item in batch:\n#             item_texts = [item['question']] + item['choices'] + [\n#                 item.get('hint', '') or \"\",\n#                 item.get('lecture', '') or \"\",\n#                 item.get('solution', '') or \"\",\n#                 item.get('skill', '') or \"\"\n#             ]\n#             texts_per_item.extend(item_texts)\n#             counts_per_item.append(len(item_texts))\n\n#         # Translate entire batch\n#         translations = translate_batch(texts_per_item, num_beams=3)\n\n#         # Map translations back to items\n#         idx = 0\n#         for item, count in zip(batch, counts_per_item):\n#             item_translations = translations[idx:idx+count]\n#             idx += count\n\n#             question, choices, hint, lecture, solution, skill = translate_item_texts(item_translations, len(item['choices']))\n\n#             translated_item = {\n#                 \"image\": item[\"image\"],\n#                 \"question\": question,\n#                 \"choices\": choices,\n#                 \"hint\": hint,\n#                 \"lecture\": lecture,\n#                 \"solution\": solution,\n#                 \"skill\": skill,\n#                 \"answer\": item[\"answer\"],\n#                 \"task\": item[\"task\"],\n#                 \"grade\": item[\"grade\"],\n#                 \"subject\": item[\"subject\"],\n#                 \"topic\": item[\"topic\"],\n#                 \"category\": item[\"category\"],\n#             }\n#             translated_items.append(translated_item)\n\n#         # Save progress after each batch (only current batch)\n#         with open(os.path.join(output_dir, \"progress.pkl\"), 'wb') as f:\n#             pickle.dump(translated_items, f)\n#         print(f\"Translated {min(start_idx+batch_size, total_items)}/{total_items} items\")\n\n#     # Combine existing + new translations\n#     all_items = existing_items + translated_items\n    \n#     # Save as HuggingFace Dataset\n#     hf_dataset = create_huggingface_dataset(all_items)\n#     hf_dataset.save_to_disk(os.path.join(output_dir, \"scienceqa_nepali_dataset\"))\n    \n#     print(f\"Translation completed! Total items: {len(all_items)} (added {len(translated_items)} new items)\")\n#     print(f\"Dataset saved to: {output_dir}/scienceqa_nepali_dataset\")\n#     return translated_items\n\n# # ----------------------------\n# # MAIN\n# # ----------------------------\n# if __name__ == \"__main__\":\n#     ds = load_dataset(\"derek-thomas/ScienceQA\")\n#     train_data = ds['train']\n\n#     selected_data = train_data.select(range(3000, len(train_data)))\n#     translated_data = translate_dataset(selected_data, max_items=100, batch_size=10)\n    \n#     print(\"Translation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T13:41:17.658231Z","iopub.execute_input":"2025-09-30T13:41:17.658515Z","iopub.status.idle":"2025-09-30T13:41:20.679857Z","shell.execute_reply.started":"2025-09-30T13:41:17.658494Z","shell.execute_reply":"2025-09-30T13:41:20.678781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T13:41:13.288239Z","iopub.execute_input":"2025-09-30T13:41:13.288759Z","iopub.status.idle":"2025-09-30T13:41:13.292318Z","shell.execute_reply.started":"2025-09-30T13:41:13.288715Z","shell.execute_reply":"2025-09-30T13:41:13.291525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(translated_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T13:37:59.385406Z","iopub.execute_input":"2025-09-30T13:37:59.385700Z","iopub.status.idle":"2025-09-30T13:37:59.390870Z","shell.execute_reply.started":"2025-09-30T13:37:59.385679Z","shell.execute_reply":"2025-09-30T13:37:59.390121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# translated_data[3000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:48:08.065592Z","iopub.execute_input":"2025-09-30T12:48:08.065878Z","iopub.status.idle":"2025-09-30T12:48:08.071583Z","shell.execute_reply.started":"2025-09-30T12:48:08.065859Z","shell.execute_reply":"2025-09-30T12:48:08.070833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data[3000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:48:48.911847Z","iopub.execute_input":"2025-09-30T12:48:48.912163Z","iopub.status.idle":"2025-09-30T12:48:48.918923Z","shell.execute_reply.started":"2025-09-30T12:48:48.912143Z","shell.execute_reply":"2025-09-30T12:48:48.918237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import load_from_disk\n\n# # Load the dataset\n# dataset = load_from_disk(\"scienceqa_nepali/scienceqa_nepali_dataset\")  # or wherever you extracted it\n\n# print(f\"Total items: {len(dataset)}\")\n# print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T13:38:13.879411Z","iopub.execute_input":"2025-09-30T13:38:13.880048Z","iopub.status.idle":"2025-09-30T13:38:13.889250Z","shell.execute_reply.started":"2025-09-30T13:38:13.880023Z","shell.execute_reply":"2025-09-30T13:38:13.888505Z"}},"outputs":[],"execution_count":null}]}